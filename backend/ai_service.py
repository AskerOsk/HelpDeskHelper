"""
AI Service - Ollama Integration for Sulpak HelpDesk
Handles intelligent customer support responses
"""

import os
import logging
from typing import Dict, List, Tuple, Optional
import httpx
from dotenv import load_dotenv
from constants import (
    AI_MODEL_DEFAULT,
    AI_CONFIDENCE_THRESHOLD,
    AI_MAX_CONTEXT_MESSAGES,
    SENDER_USER,
    SENDER_AI,
    AI_RESPONSE_TIMEOUT
)

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)


class AIService:
    """AI-powered customer support assistant using Ollama"""

    def __init__(self):
        """Initialize AI service with Ollama client"""
        self.ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
        self.model = os.getenv('AI_MODEL', 'llama3.2:latest')
        self.use_ollama = os.getenv('USE_OLLAMA', 'true').lower() == 'true'

        # For future Anthropic support
        self.anthropic_key = os.getenv('ANTHROPIC_API_KEY')

        if self.use_ollama:
            logger.info(f"AI Service initialized with Ollama | URL: {self.ollama_url} | Model: {self.model}")
        else:
            if not self.anthropic_key:
                raise ValueError("ANTHROPIC_API_KEY not found in environment variables")
            logger.info(f"AI Service initialized with Anthropic | Model: {self.model}")

    def _build_system_prompt(self) -> str:
        """Build system prompt for AI assistant"""
        return """–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ Sulpak (–∫—Ä—É–ø–Ω–µ–π—à–∞—è —Å–µ—Ç—å —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∏ –∏ –±—ã—Ç–æ–≤–æ–π —Ç–µ—Ö–Ω–∏–∫–∏ –≤ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ).

–¢–í–û–Ø –†–û–õ–¨:
- –ü–æ–º–æ–≥–∞–π –∫–ª–∏–µ–Ω—Ç–∞–º —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –æ –∑–∞–∫–∞–∑–∞—Ö, –¥–æ—Å—Ç–∞–≤–∫–µ, –æ–ø–ª–∞—Ç–µ, –≤–æ–∑–≤—Ä–∞—Ç–µ —Ç–æ–≤–∞—Ä–æ–≤
- –û—Ç–≤–µ—á–∞–π –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ —É–º–µ—Ä–µ–Ω–Ω–æ –¥–ª—è —Ç–µ–ø–ª–æ—Ç—ã –æ–±—â–µ–Ω–∏—è
- –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ –∏–ª–∏ –≤–æ–ø—Ä–æ—Å —Å–ª–æ–∂–Ω—ã–π ‚Äî —á–µ—Å—Ç–Ω–æ –ø—Ä–∏–∑–Ω–∞–π —ç—Ç–æ

–ü–û–õ–ò–¢–ò–ö–ò SULPAK:
- –î–æ—Å—Ç–∞–≤–∫–∞: –ü–æ –ê–ª–º–∞—Ç—ã 1-2 –¥–Ω—è, –ø–æ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω—É 3-7 –¥–Ω–µ–π
- –í–æ–∑–≤—Ä–∞—Ç: 14 –¥–Ω–µ–π —Å –º–æ–º–µ–Ω—Ç–∞ –ø–æ–∫—É–ø–∫–∏, —Ç–æ–≤–∞—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —É–ø–∞–∫–æ–≤–∫–µ
- –ì–∞—Ä–∞–Ω—Ç–∏—è: –°–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—é (–æ–±—ã—á–Ω–æ 12-24 –º–µ—Å—è—Ü–∞)
- –û–ø–ª–∞—Ç–∞: –ù–∞–ª–∏—á–Ω—ã–µ, –∫–∞—Ä—Ç–∞, Kaspi Red, —Ä–∞—Å—Å—Ä–æ—á–∫–∞

–ö–û–ì–î–ê –ù–£–ñ–ï–ù –ú–ï–ù–ï–î–ñ–ï–†:
- –ö–ª–∏–µ–Ω—Ç –æ—á–µ–Ω—å –Ω–µ–¥–æ–≤–æ–ª–µ–Ω/—Ä–∞—Å—Å—Ç—Ä–æ–µ–Ω/–∞–≥—Ä–µ—Å—Å–∏–≤–µ–Ω (–Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å)
- –í–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å–∏—Å—Ç–µ–º–∞–º (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–∫–∞–∑–∞ –ø–æ –Ω–æ–º–µ—Ä—É)
- –ó–∞–ø—Ä–æ—Å –Ω–∞ –≤–æ–∑–≤—Ä–∞—Ç –¥–µ–Ω–µ–≥ –∏–ª–∏ –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—é
- –ö–ª–∏–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—é –∑–∞ –º–æ—Ä–∞–ª—å–Ω—ã–π —É—â–µ—Ä–± –∏–ª–∏ –∏—Å–ø–æ—Ä—á–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —Å–ª–æ–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ –ø—Ä–æ–¥—É–∫—Ç–µ
- –ö–ª–∏–µ–Ω—Ç —è–≤–Ω–æ –ø—Ä–æ—Å–∏—Ç –∂–∏–≤–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ü–†–ê–í–ò–õ–ê –≠–°–ö–ê–õ–ê–¶–ò–ò:

üö´ –ó–ê–ü–†–ï–©–ï–ù–û:
- –ù–ï –¥–∞–≤–∞–π –∫–ª–∏–µ–Ω—Ç—É email –∏–ª–∏ –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –¥–ª—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏—è
- –ù–ï –≥–æ–≤–æ—Ä–∏ "–Ω–∞–ø–∏—à–∏—Ç–µ –Ω–∞ –ø–æ—á—Ç—É support@..."
- –ù–ï –≥–æ–≤–æ—Ä–∏ "–ø–æ–∑–≤–æ–Ω–∏—Ç–µ –ø–æ –Ω–æ–º–µ—Ä—É..."
- –ù–ï –≥–æ–≤–æ—Ä–∏ "—è —É–∂–µ –æ—Ç–ø—Ä–∞–≤–∏–ª –≤–∞—à –∑–∞–ø—Ä–æ—Å"
- –ù–ï –¥–µ–ª–∞–π –≤–∏–¥ —á—Ç–æ —É–∂–µ —á—Ç–æ-—Ç–æ —Å–¥–µ–ª–∞–ª

‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô –°–ü–û–°–û–ë:
1. –ï—Å–ª–∏ —Å–∏—Ç—É–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –º–µ–Ω–µ–¥–∂–µ—Ä–∞, —Å–ø—Ä–æ—Å–∏:
   "–•–æ—Ç–∏—Ç–µ, —á—Ç–æ–±—ã —è –ø–µ—Ä–µ–¥–∞–ª –≤–∞—à –≤–æ–ø—Ä–æ—Å –Ω–∞—à–µ–º—É —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É? –ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–ª—É—á–∏—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∏ —Å–≤—è–∂–µ—Ç—Å—è —Å –≤–∞–º–∏."

2. –î–æ–∂–¥–∏—Å—å –æ—Ç–≤–µ—Ç–∞ –∫–ª–∏–µ–Ω—Ç–∞ (–î–∞/–ù–µ—Ç)

3. –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç —Å–æ–≥–ª–∞—Å–µ–Ω (–î–∞), —Å–∫–∞–∂–∏:
   "–ü–µ—Ä–µ–¥–∞—é –≤–∞—à –∑–∞–ø—Ä–æ—Å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É. –ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–ª—É—á–∏—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∏ —Å–≤—è–∂–µ—Ç—Å—è —Å –≤–∞–º–∏ –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è."

4. –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –æ—Ç–∫–∞–∑–∞–ª—Å—è (–ù–µ—Ç), –ø—Ä–æ–¥–æ–ª–∂–∞–π –ø–æ–º–æ–≥–∞—Ç—å —Å–∞–º

–ü–†–ò–ú–ï–† –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –î–ò–ê–õ–û–ì–ê:
–ö–ª–∏–µ–Ω—Ç: "–•–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ —Å–ª–æ–º–∞–ª—Å—è! –¢—Ä–µ–±—É—é –≤–æ–∑–≤—Ä–∞—Ç –¥–µ–Ω–µ–≥!"
–¢—ã: "–ü–æ–Ω–∏–º–∞—é –≤–∞—à–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ. –≠—Ç–æ —Å–µ—Ä—å–µ–∑–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–µ–±—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è. –•–æ—Ç–∏—Ç–µ, —á—Ç–æ–±—ã —è –ø–µ—Ä–µ–¥–∞–ª –≤–∞—à –≤–æ–ø—Ä–æ—Å –Ω–∞—à–µ–º—É —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É? –ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–ª—É—á–∏—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∏ —Å–≤—è–∂–µ—Ç—Å—è —Å –≤–∞–º–∏."
–ö–ª–∏–µ–Ω—Ç: "–î–∞"
–¢—ã: "–ü–µ—Ä–µ–¥–∞—é –≤–∞—à –∑–∞–ø—Ä–æ—Å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É. –ú–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–ª—É—á–∏—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∏ —Å–≤—è–∂–µ—Ç—Å—è —Å –≤–∞–º–∏ –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è."

–°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø:
- –ö—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –Ω–µ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã
- –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ ‚Äî –∑–∞–¥–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å
- –ó–∞–∫—Ä—ã–≤–∞–π –æ–±—Ä–∞—â–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º "–ï—Å—Ç—å –µ—â—ë –≤–æ–ø—Ä–æ—Å—ã?" –∫–æ–≥–¥–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Ä–µ—à–µ–Ω–∞

–ü–æ–º–Ω–∏: —Ç–≤–æ—è —Ü–µ–ª—å ‚Äî –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç—É –±—ã—Å—Ç—Ä–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ! üöÄ"""

    def _build_conversation_context(self, messages: List[Dict], for_ollama: bool = True) -> List[Dict]:
        """
        Build conversation context for AI API

        Args:
            messages: List of message dicts with 'sender_type' and 'content'
            for_ollama: If True, format for Ollama, else for Anthropic

        Returns:
            List of messages in API format
        """
        # Limit context to recent messages
        recent_messages = messages[-AI_MAX_CONTEXT_MESSAGES:]

        formatted_messages = []
        for msg in recent_messages:
            role = "user" if msg['sender_type'] == SENDER_USER else "assistant"
            content = msg['content']

            # Add media context if present
            if msg.get('media_type'):
                media_type = msg['media_type']
                content = f"[{media_type.upper()}] {content if content else '–§–æ—Ç–æ/–≤–∏–¥–µ–æ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è'}"

            formatted_messages.append({
                "role": role,
                "content": content
            })

        return formatted_messages

    def _calculate_confidence(self, response: str) -> float:
        """
        Calculate AI confidence score based on response content

        Args:
            response: AI-generated response text

        Returns:
            Confidence score between 0 and 1
        """
        # Simple heuristic-based confidence scoring
        confidence = 1.0

        # Lower confidence indicators
        uncertainty_phrases = [
            '–Ω–µ —É–≤–µ—Ä–µ–Ω',
            '–Ω–µ –∑–Ω–∞—é',
            '–≤–æ–∑–º–æ–∂–Ω–æ',
            '–º–æ–∂–µ—Ç –±—ã—Ç—å',
            '—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ',
            '–ø–æ–ø—Ä–æ–±—É–π—Ç–µ',
            '—Å–≤—è–∂–∏—Ç–µ—Å—å —Å',
            '–æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –º–µ–Ω–µ–¥–∂–µ—Ä—É'
        ]

        response_lower = response.lower()
        for phrase in uncertainty_phrases:
            if phrase in response_lower:
                confidence -= 0.15

        # Very short responses might indicate uncertainty
        if len(response) < 50:
            confidence -= 0.1

        # Question-heavy responses indicate need for more info
        question_count = response.count('?')
        if question_count > 2:
            confidence -= 0.1

        return max(0.1, min(1.0, confidence))

    def _should_escalate(self, response: str, confidence: float) -> bool:
        """
        Determine if conversation should be escalated to human manager

        Args:
            response: AI-generated response
            confidence: AI confidence score

        Returns:
            True if escalation needed
        """
        # Only escalate when user CONFIRMS they want a manager
        # and AI says it's transferring to specialist

        # Explicit escalation confirmations (AI says this AFTER user confirms)
        escalation_triggers = [
            '–ø–µ—Ä–µ–¥–∞—é –≤–∞—à –∑–∞–ø—Ä–æ—Å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É',
            '–ø–µ—Ä–µ–¥–∞–º –≤–∞—à –∑–∞–ø—Ä–æ—Å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É',
            '–ø–µ—Ä–µ–¥–∞—é —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É',
            '–ø–µ—Ä–µ–¥–∞–º —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É',
            '–ø–µ—Ä–µ–¥–∞–Ω–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É',
            '–ø–µ—Ä–µ–¥–∞—é –º–µ–Ω–µ–¥–∂–µ—Ä—É',
            '–ø–µ—Ä–µ–¥–∞–º –º–µ–Ω–µ–¥–∂–µ—Ä—É',
            '–ø–µ—Ä–µ–¥–∞–Ω–æ –º–µ–Ω–µ–¥–∂–µ—Ä—É',
            '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ–ª—É—á–∏–ª',
            '–º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–ª—É—á–∏–ª'
        ]

        response_lower = response.lower()
        for trigger in escalation_triggers:
            if trigger in response_lower:
                logger.info(f"Escalating due to user confirmation: {trigger}")
                return True

        return False

    async def _call_ollama(
        self,
        system_prompt: str,
        messages: List[Dict]
    ) -> str:
        """
        Call Ollama API

        Args:
            system_prompt: System prompt
            messages: Conversation messages

        Returns:
            AI response text
        """
        # Build full conversation with system prompt
        full_messages = [{"role": "system", "content": system_prompt}] + messages

        async with httpx.AsyncClient(timeout=AI_RESPONSE_TIMEOUT) as client:
            response = await client.post(
                f"{self.ollama_url}/api/chat",
                json={
                    "model": self.model,
                    "messages": full_messages,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": 1024
                    }
                }
            )
            response.raise_for_status()
            data = response.json()
            return data['message']['content']

    async def _call_anthropic(
        self,
        system_prompt: str,
        messages: List[Dict]
    ) -> str:
        """
        Call Anthropic API (for future use)

        Args:
            system_prompt: System prompt
            messages: Conversation messages

        Returns:
            AI response text
        """
        from anthropic import AsyncAnthropic
        client = AsyncAnthropic(api_key=self.anthropic_key)

        response = await client.messages.create(
            model=self.model,
            max_tokens=1024,
            system=system_prompt,
            messages=messages,
            temperature=0.7
        )

        return response.content[0].text

    async def get_ai_response(
        self,
        ticket_id: int,
        conversation_history: List[Dict],
        user_info: Dict
    ) -> Tuple[str, float, bool]:
        """
        Generate AI response for user message

        Args:
            ticket_id: Ticket ID for logging
            conversation_history: Full conversation history
            user_info: User information (username, user_id)

        Returns:
            Tuple of (response_text, confidence_score, should_escalate)
        """
        try:
            logger.info(f"Generating AI response for ticket #{ticket_id} | History: {len(conversation_history)} messages")

            # Debug: log conversation
            for i, msg in enumerate(conversation_history):
                logger.debug(f"  Msg {i+1}: {msg['sender_type']} - {msg['content'][:50]}...")

            # Build context
            system_prompt = self._build_system_prompt()
            messages = self._build_conversation_context(conversation_history)

            # Call appropriate AI service
            if self.use_ollama:
                response_text = await self._call_ollama(system_prompt, messages)
            else:
                response_text = await self._call_anthropic(system_prompt, messages)

            # Calculate confidence and escalation
            confidence = self._calculate_confidence(response_text)
            should_escalate = self._should_escalate(response_text, confidence)

            logger.info(
                f"AI response generated for ticket #{ticket_id} | "
                f"Confidence: {confidence:.2f} | "
                f"Escalate: {should_escalate}"
            )

            return response_text, confidence, should_escalate

        except Exception as e:
            logger.error(f"Error generating AI response for ticket #{ticket_id}: {e}", exc_info=True)
            # Fallback response on error
            fallback_response = (
                "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞. "
                "–Ø –ø–µ—Ä–µ–¥–∞–º –≤–∞—à –≤–æ–ø—Ä–æ—Å –º–µ–Ω–µ–¥–∂–µ—Ä—É, –∫–æ—Ç–æ—Ä—ã–π —Å–≤—è–∂–µ—Ç—Å—è —Å –≤–∞–º–∏ –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è. üôè"
            )
            return fallback_response, 0.0, True  # Always escalate on error

    async def generate_conversation_summary(
        self,
        conversation_history: List[Dict]
    ) -> str:
        """
        Generate concise summary of conversation for email notification

        Args:
            conversation_history: Full conversation history

        Returns:
            Summary text in Russian
        """
        try:
            # Build conversation text
            conversation_text = "\n\n".join([
                f"{'–ö–ª–∏–µ–Ω—Ç' if msg['sender_type'] == SENDER_USER else 'AI'}: {msg['content']}"
                for msg in conversation_history
            ])

            # Create summarization prompt
            summary_prompt = f"""–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) —ç—Ç–æ–π –±–µ—Å–µ–¥—ã —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:

{conversation_text}

–£–∫–∞–∂–∏:
1. –°—É—Ç—å –ø—Ä–æ–±–ª–µ–º—ã/–≤–æ–ø—Ä–æ—Å–∞ –∫–ª–∏–µ–Ω—Ç–∞
2. –ß—Ç–æ –±—ã–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ AI
3. –ü–æ—á–µ–º—É —Ç—Ä–µ–±—É–µ—Ç—Å—è —ç—Å–∫–∞–ª–∞—Ü–∏—è

–§–æ—Ä–º–∞—Ç: —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ, –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤."""

            messages = [{"role": "user", "content": summary_prompt}]

            # Call appropriate AI service
            if self.use_ollama:
                async with httpx.AsyncClient(timeout=AI_RESPONSE_TIMEOUT) as client:
                    response = await client.post(
                        f"{self.ollama_url}/api/chat",
                        json={
                            "model": self.model,
                            "messages": messages,
                            "stream": False,
                            "options": {
                                "temperature": 0.5,
                                "num_predict": 512
                            }
                        }
                    )
                    response.raise_for_status()
                    data = response.json()
                    summary = data['message']['content'].strip()
            else:
                from anthropic import AsyncAnthropic
                client = AsyncAnthropic(api_key=self.anthropic_key)
                response = await client.messages.create(
                    model=self.model,
                    max_tokens=512,
                    messages=messages,
                    temperature=0.5
                )
                summary = response.content[0].text.strip()

            logger.info(f"Generated conversation summary: {summary[:100]}...")
            return summary

        except Exception as e:
            logger.error(f"Error generating summary: {e}", exc_info=True)
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—é–º–µ –±–µ—Å–µ–¥—ã."


# Global AI service instance
_ai_service: Optional[AIService] = None


def get_ai_service() -> AIService:
    """Get or create AI service singleton"""
    global _ai_service
    if _ai_service is None:
        _ai_service = AIService()
    return _ai_service
